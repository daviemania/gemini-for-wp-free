# github-chat.prompt.yml
# Configuration and guidance for GitHub Models Chat interface

project:
    name: "GitHub Models Chat"
    description: "Advanced chat interface for GitHub Models API with streaming, conversation management, and multi-model support"
    script: "chat-github.js"
    npm_command: "npm run githubchat"

## Configuration
config:
    env_var: "GITHUB_MODELS_TOKEN"
    endpoint: "https://models.github.ai/inference"
    permissions_required: "models:read"
    conversations_dir: "./conversations/"

## Available Models
models:
    reasoning:
        - id: "o1-preview"
          name: "OpenAI o1 Preview"
          description: "Advanced reasoning model"
          best_for: "Complex problem solving, logic, math"

        - id: "o1-mini"
          name: "OpenAI o1 Mini"
          description: "Efficient reasoning model"
          best_for: "Quick reasoning tasks"

    chat:
        - id: "gpt-4o"
          name: "GPT-4o"
          description: "Most capable GPT-4 model"
          best_for: "General purpose, coding, analysis"

        - id: "gpt-4o-mini"
          name: "GPT-4o Mini"
          description: "Fast and efficient"
          best_for: "Quick responses, everyday tasks"
          default: true

    coding:
        - id: "deepseek/DeepSeek-V3-0324"
          name: "DeepSeek V3"
          description: "Specialized coding model"
          best_for: "Code generation, debugging, technical tasks"

        - id: "Phi-4"
          name: "Microsoft Phi-4"
          description: "Compact but powerful"
          best_for: "Efficient coding and reasoning"

    open_source:
        - id: "Meta-Llama-3.1-405B-Instruct"
          name: "Llama 3.1 405B"
          description: "Largest Llama model"
          best_for: "Complex tasks, long context"

        - id: "Meta-Llama-3.1-70B-Instruct"
          name: "Llama 3.1 70B"
          description: "Balanced performance"
          best_for: "General purpose, good quality"

        - id: "Meta-Llama-3.3-70B-Instruct"
          name: "Llama 3.3 70B"
          description: "Latest Llama release"
          best_for: "Improved reasoning and coding"

    specialized:
        - id: "Mistral-large-2411"
          name: "Mistral Large"
          description: "High-performance European model"
          best_for: "Multilingual, coding, reasoning"

        - id: "Mistral-small"
          name: "Mistral Small"
          description: "Efficient Mistral model"
          best_for: "Quick tasks, cost-effective"

        - id: "Mistral-Nemo"
          name: "Mistral Nemo"
          description: "Compact Mistral variant"
          best_for: "Fast responses"

        - id: "cohere-command-r"
          name: "Cohere Command R"
          description: "RAG-optimized model"
          best_for: "Search and retrieval tasks"

        - id: "cohere-command-r-plus"
          name: "Cohere Command R+"
          description: "Enhanced RAG model"
          best_for: "Complex retrieval and analysis"

        - id: "AI21-Jamba-1.5-Large"
          name: "AI21 Jamba 1.5 Large"
          description: "Hybrid architecture model"
          best_for: "Long context, efficiency"

        - id: "AI21-Jamba-1.5-Mini"
          name: "AI21 Jamba 1.5 Mini"
          description: "Compact Jamba model"
          best_for: "Fast, efficient tasks"

## Features
features:
    streaming:
        enabled: true
        description: "Real-time response streaming like ChatGPT"
        toggle: "Available in /settings menu"

    conversation_management:
        save: "Automatically saves to conversations/ directory"
        load: "Load previous conversations from file"
        format: "JSON with timestamps and metadata"

    model_switching:
        method: "Hot-swap during conversation"
        access: "Via /settings menu"
        persistence: "Model choice maintained in session"

    custom_prompts:
        system_prompt: "Customize AI behavior and personality"
        access: "Via /settings menu"
        examples:
            - "You are an expert WordPress developer"
            - "You are a technical writer specializing in developer documentation"
            - "You are a code reviewer focused on security and performance"

    statistics:
        tracked:
            - "Message count (user/assistant)"
            - "Total characters"
            - "Current model"
            - "Streaming status"

## Commands
commands:
    chat:
        description: "Type any message to chat with AI"

    "/settings":
        description: "Open settings menu"
        options:
            - "Change model"
            - "Toggle streaming"
            - "Change system prompt"
            - "View conversation stats"
            - "Clear conversation history"
            - "Save conversation"
            - "Load conversation"

    "/save":
        description: "Quick save current conversation"
        output: "conversations/conversation-YYYY-MM-DD-HH-MM-SS.json"

    "/load":
        description: "Load a previous conversation"
        shows: "Interactive list of saved conversations"

    "/clear":
        description: "Clear conversation history"
        confirmation: "Asks for confirmation before clearing"

    "/exit":
        description: "Exit the chat application"
        aliases: ["exit", "quit"]

## Use Cases by Model
use_cases:
    wordpress_development:
        recommended_models:
            - "gpt-4o (best quality)"
            - "deepseek/DeepSeek-V3-0324 (specialized)"
            - "Meta-Llama-3.3-70B-Instruct (open source)"
        system_prompt: "You are an expert WordPress developer with deep knowledge of hooks, filters, and best practices."

    technical_writing:
        recommended_models:
            - "gpt-4o"
            - "Meta-Llama-3.1-70B-Instruct"
        system_prompt: "You are a technical writer specializing in clear, comprehensive developer documentation."

    code_review:
        recommended_models:
            - "gpt-4o"
            - "deepseek/DeepSeek-V3-0324"
            - "o1-preview (complex logic)"
        system_prompt: "You are a senior code reviewer focused on security, performance, and maintainability."

    quick_questions:
        recommended_models:
            - "gpt-4o-mini (default, fast)"
            - "Phi-4 (efficient)"
            - "Mistral-Nemo (quick)"

    ai_integration:
        recommended_models:
            - "gpt-4o"
            - "Meta-Llama-3.1-405B-Instruct"
        system_prompt: "You are an AI integration specialist helping with API implementations and agent architectures."

## Troubleshooting
troubleshooting:
    unauthorized_401:
        error: "401 Unauthorized"
        cause: "Token missing models:read permission"
        solution: |
            1. Go to GitHub Settings > Developer settings > Personal access tokens
            2. Create new token with 'models:read' permission
            3. Set as GITHUB_MODELS_TOKEN environment variable
            4. Restart the application

    streaming_issues:
        error: "Garbled or incomplete text"
        cause: "Stream buffer not properly handled"
        solution: "Fixed in latest version with proper buffer management"
        status: "Resolved"

    missing_conversations:
        error: "No saved conversations found"
        cause: "conversations/ directory doesn't exist"
        solution: "Directory is auto-created on first save"

    model_not_available:
        error: "Model returns error or unavailable"
        cause: "Model may be temporarily unavailable or requires payment"
        solution: "Try a different model or check GitHub Models status page"

## Best Practices
best_practices:
    token_security:
        - "Never commit GITHUB_MODELS_TOKEN to repository"
        - "Use .env file for local development"
        - "Set environment variable in production"
        - "Rotate tokens periodically"

    model_selection:
        - "Use gpt-4o-mini for quick iterations"
        - "Use gpt-4o or DeepSeek-V3 for production code"
        - "Use o1-preview for complex reasoning"
        - "Test with multiple models for important tasks"

    conversation_management:
        - "Save important coding sessions"
        - "Use descriptive system prompts"
        - "Clear history when switching contexts"
        - "Load previous conversations for continuity"

    performance:
        - "Enable streaming for better UX"
        - "Use smaller models for simple queries"
        - "Monitor token usage with /settings stats"
        - "Break complex tasks into smaller conversations"

## Integration with Main Project
integration:
    package_json:
        script: '"githubchat": "node chat-github.js"'

    dependencies:
        required:
            - "@azure-rest/ai-inference"
            - "@azure/core-auth"
            - "@azure/core-sse"
            - "inquirer"
            - "chalk"
            - "fs-extra"

    environment:
        required:
            - "GITHUB_MODELS_TOKEN"
        optional:
            - "GITHUB_TOKEN (for repo operations, separate)"

    directory_structure:
        - "chat-github.js (main script)"
        - "conversations/ (auto-created for saves)"
        - "github-chat.prompt.yml (this file)"

## Development Notes
development:
    architecture:
        streaming: "Uses Server-Sent Events (SSE) with proper buffer management"
        error_handling: "Comprehensive try-catch with user-friendly messages"
        state_management: "In-memory conversation history with file persistence"

    future_enhancements:
        - "File upload support for code review"
        - "Syntax highlighting for code blocks"
        - "Export conversations to Markdown"
        - "Integration with git for commit message generation"
        - "Voice input/output support"
        - "Collaborative chat sessions"

    known_limitations:
        - "No file upload capability (yet)"
        - "Conversations stored locally only"
        - "No web interface (CLI only)"
        - "Single user per session"

## Quick Reference
quick_reference:
    start: "npm run githubchat"
    save: "Type /save or use /settings menu"
    change_model: "Type /settings > Change Model"
    best_coding_model: "deepseek/DeepSeek-V3-0324 or gpt-4o"
    fastest_model: "gpt-4o-mini or Phi-4"
    most_powerful: "gpt-4o or Meta-Llama-3.1-405B-Instruct"
